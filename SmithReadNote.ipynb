{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0005d3f1-2078-41d0-ac93-1ba3eaa997f5",
   "metadata": {},
   "source": [
    "# Transformers 筆記\n",
    "這是閱讀`https://github.com/zyds/transformers-code`的筆記心得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ba5e6-5ff2-475f-9ff8-cc4803fdac14",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0673e62b-1b08-4ea1-be63-91071d102314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['audio-classification', 'automatic-speech-recognition', 'text-to-audio', 'feature-extraction', 'text-classification', 'token-classification', 'question-answering', 'table-question-answering', 'visual-question-answering', 'document-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-audio-classification', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-text', 'object-detection', 'zero-shot-object-detection', 'depth-estimation', 'video-classification', 'mask-generation', 'image-to-image'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "SUPPORTED_TASKS.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e23dc6-90f1-4b46-b306-5a4acb3fd8ea",
   "metadata": {},
   "source": [
    "| Pipeline Type                        | Description                                    |\n",
    "|--------------------------------------|------------------------------------------------|\n",
    "| audio-classification                 | will return a AudioClassificationPipeline      |\n",
    "| automatic-speech-recognition         | will return a AutomaticSpeechRecognitionPipeline|\n",
    "| depth-estimation                     | will return a DepthEstimationPipeline          |\n",
    "| document-question-answering          | will return a DocumentQuestionAnsweringPipeline|\n",
    "| feature-extraction                   | will return a FeatureExtractionPipeline        |\n",
    "| fill-mask                            | will return a FillMaskPipeline                 |\n",
    "| image-classification                 | will return a ImageClassificationPipeline      |\n",
    "| image-feature-extraction             | will return an ImageFeatureExtractionPipeline  |\n",
    "| image-segmentation                   | will return a ImageSegmentationPipeline        |\n",
    "| image-to-image                       | will return a ImageToImagePipeline             |\n",
    "| image-to-text                        | will return a ImageToTextPipeline              |\n",
    "| mask-generation                      | will return a MaskGenerationPipeline           |\n",
    "| object-detection                     | will return a ObjectDetectionPipeline          |\n",
    "| question-answering                   | will return a QuestionAnsweringPipeline        |\n",
    "| summarization                        | will return a SummarizationPipeline            |\n",
    "| table-question-answering             | will return a TableQuestionAnsweringPipeline   |\n",
    "| text2text-generation                 | will return a Text2TextGenerationPipeline      |\n",
    "| text-classification (sentiment-analysis) | will return a TextClassificationPipeline    |\n",
    "| text-generation                      | will return a TextGenerationPipeline           |\n",
    "| text-to-audio (text-to-speech)       | will return a TextToAudioPipeline              |\n",
    "| token-classification (ner)           | will return a TokenClassificationPipeline      |\n",
    "| translation                          | will return a TranslationPipeline              |\n",
    "| translation_xx_to_yy                 | will return a TranslationPipeline              |\n",
    "| video-classification                 | will return a VideoClassificationPipeline      |\n",
    "| visual-question-answering            | will return a VisualQuestionAnsweringPipeline  |\n",
    "| zero-shot-classification             | will return a ZeroShotClassificationPipeline   |\n",
    "| zero-shot-image-classification       | will return a ZeroShotImageClassificationPipeline|\n",
    "| zero-shot-audio-classification       | will return a ZeroShotAudioClassificationPipeline|\n",
    "| zero-shot-object-detection           | will return a ZeroShotObjectDetectionPipeline  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb714be-5e1b-4086-b66e-8677108fe571",
   "metadata": {},
   "source": [
    "根據task得到不同的pipeline，每種有各自事先定義好的輸入輸出。\n",
    "可以參閱各自的`__call__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bacca2-3597-4a3f-b8a1-ffd1ecf78b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative (stars 1, 2 and 3)', 'score': 0.5998563766479492}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", \n",
    "                model=\"uer/roberta-base-finetuned-dianping-chinese\", #不指定也會有預設值\n",
    "                 device=0\n",
    "                #, or deivce_map='auto'\n",
    "               )\n",
    "pipe(\"This restaurant is awesome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96897038-8be6-45a9-bbff-d9f7d48f33af",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ba299f-ca60-418a-9987-9330031dd804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54666bf4-0267-43a9-9f76-068dfd56a32b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'訕': 6247,\n",
       " '戦': 2778,\n",
       " '惦': 2671,\n",
       " '抨': 2846,\n",
       " '##ti': 9007,\n",
       " '##质': 19631,\n",
       " '##钜': 20218,\n",
       " '##驹': 20784,\n",
       " '##僅': 14063,\n",
       " 'audio': 11698,\n",
       " '繞': 5254,\n",
       " 'rc': 11746,\n",
       " '薨': 5957,\n",
       " '1v': 12819,\n",
       " 'delete': 12845,\n",
       " '透': 6851,\n",
       " 'hdr': 10465,\n",
       " '##º': 13358,\n",
       " '倉': 942,\n",
       " '##赋': 19659,\n",
       " '##lie': 10158,\n",
       " '##つ': 9775,\n",
       " '801': 12566,\n",
       " '簌': 5078,\n",
       " '##苣': 18791,\n",
       " '##lton': 10377,\n",
       " '##cg': 11478,\n",
       " '锲': 7244,\n",
       " 'pass': 9703,\n",
       " 'save': 13069,\n",
       " '京': 776,\n",
       " '花': 5709,\n",
       " '##孱': 15173,\n",
       " '踮': 6680,\n",
       " '##掲': 16034,\n",
       " '釀': 7021,\n",
       " '##ᅪ': 13476,\n",
       " '##580': 10703,\n",
       " '##弾': 15546,\n",
       " '飄': 7597,\n",
       " '鳝': 7850,\n",
       " 'ai': 8578,\n",
       " '##渤': 17000,\n",
       " '##们': 13869,\n",
       " '##嘖': 14711,\n",
       " '收': 3119,\n",
       " '##и': 11000,\n",
       " '##牟': 17340,\n",
       " '##社': 17909,\n",
       " '！': 8013,\n",
       " '颅': 7565,\n",
       " 'wikia': 8708,\n",
       " 'チ': 609,\n",
       " '舀': 5641,\n",
       " '##晞': 16300,\n",
       " '##飛': 20663,\n",
       " '到': 1168,\n",
       " '##权': 16383,\n",
       " '控': 2971,\n",
       " '鞑': 7493,\n",
       " '##くれる': 11241,\n",
       " '##し': 8733,\n",
       " '##阈': 20385,\n",
       " 'vr': 8260,\n",
       " 'menu': 8888,\n",
       " '髯': 7774,\n",
       " '辩': 6796,\n",
       " '慄': 2704,\n",
       " '##猝': 17397,\n",
       " '##覬': 19275,\n",
       " '##應': 15803,\n",
       " '膩': 5611,\n",
       " '潴': 4062,\n",
       " 'said': 8385,\n",
       " '錄': 7087,\n",
       " '##勇': 14292,\n",
       " '103': 8615,\n",
       " '東': 3346,\n",
       " '##描': 16046,\n",
       " '焉': 4183,\n",
       " 'f4': 11464,\n",
       " '##倦': 14015,\n",
       " '##瑠': 17507,\n",
       " '咯': 1492,\n",
       " '##卷': 14375,\n",
       " '僕': 1010,\n",
       " '嘆': 1647,\n",
       " '（': 8020,\n",
       " '77': 8411,\n",
       " '##ide': 9552,\n",
       " 'labels': 11562,\n",
       " '##ᵃ': 13489,\n",
       " '##皓': 17702,\n",
       " '##荆': 18826,\n",
       " '篓': 5066,\n",
       " '##左': 15397,\n",
       " '##领': 20623,\n",
       " '##懑': 15806,\n",
       " '墮': 1876,\n",
       " '儕': 1028,\n",
       " '姹': 2011,\n",
       " '墾': 1879,\n",
       " '碳': 4823,\n",
       " '鴛': 7859,\n",
       " '##鄙': 20025,\n",
       " '##埼': 14883,\n",
       " '##攬': 16174,\n",
       " '##服': 16359,\n",
       " '##uard': 12589,\n",
       " '##沧': 16828,\n",
       " '##ｕ': 21098,\n",
       " '欽': 3620,\n",
       " '##•': 13499,\n",
       " '##桁': 16480,\n",
       " '[unused19]': 19,\n",
       " '戌': 2764,\n",
       " '醍': 7006,\n",
       " '##坯': 14848,\n",
       " '##或': 15829,\n",
       " '##璁': 17517,\n",
       " '##痢': 17638,\n",
       " 'articles': 12431,\n",
       " '##誣': 19355,\n",
       " '##餛': 20685,\n",
       " '##怎': 15639,\n",
       " '脛': 5559,\n",
       " '梳': 3463,\n",
       " '沃': 3753,\n",
       " 'ftp': 9195,\n",
       " '享': 775,\n",
       " '鳄': 7843,\n",
       " '##墓': 14924,\n",
       " '槍': 3541,\n",
       " '噹': 1698,\n",
       " '1200': 8552,\n",
       " '調': 6310,\n",
       " '##肏': 18549,\n",
       " 'zip': 10947,\n",
       " '##乡': 13797,\n",
       " 'policy': 8659,\n",
       " '##蜓': 19109,\n",
       " '嚏': 1704,\n",
       " '##bbs': 12646,\n",
       " '纜': 5271,\n",
       " '弔': 2470,\n",
       " '##偃': 14025,\n",
       " '缘': 5357,\n",
       " '钏': 7155,\n",
       " '[unused23]': 23,\n",
       " '##烊': 17222,\n",
       " '毂': 3674,\n",
       " '圩': 1763,\n",
       " 'march': 9581,\n",
       " '寻': 2192,\n",
       " '##54': 9488,\n",
       " '##语': 19484,\n",
       " '讹': 6388,\n",
       " 'ul': 10293,\n",
       " '輛': 6739,\n",
       " '##nter': 11526,\n",
       " '##蕈': 18989,\n",
       " '##邓': 19981,\n",
       " '戸': 2788,\n",
       " '##tags': 11313,\n",
       " '她': 1961,\n",
       " '省': 4689,\n",
       " '##睦': 17778,\n",
       " '競': 5000,\n",
       " '##ery': 11041,\n",
       " '##咽': 14555,\n",
       " '##ark': 11153,\n",
       " '##姜': 15059,\n",
       " 'dlc': 12439,\n",
       " '勾': 1256,\n",
       " '桧': 3443,\n",
       " '##临': 13764,\n",
       " '霄': 7446,\n",
       " '##札': 16373,\n",
       " 'jay': 12541,\n",
       " '##贼': 19649,\n",
       " '〓': 525,\n",
       " '[unused31]': 31,\n",
       " '##浣': 16911,\n",
       " '##eg': 10935,\n",
       " '賄': 6535,\n",
       " '##尧': 15273,\n",
       " 'vc': 9438,\n",
       " '##can': 12632,\n",
       " '##窘': 18028,\n",
       " 'ba': 10322,\n",
       " '##騎': 20754,\n",
       " '##吡': 14471,\n",
       " 'mit': 9315,\n",
       " '##nt': 8511,\n",
       " '崑': 2302,\n",
       " '拗': 2871,\n",
       " '槌': 3540,\n",
       " '骑': 7744,\n",
       " '乡': 740,\n",
       " '緘': 5220,\n",
       " 'ﾗ': 8095,\n",
       " '##云': 13813,\n",
       " '##垠': 14859,\n",
       " '##屿': 15314,\n",
       " '##沪': 16829,\n",
       " '綴': 5207,\n",
       " '##燊': 17295,\n",
       " '殓': 3657,\n",
       " '拼': 2894,\n",
       " '戛': 2775,\n",
       " 'かある': 10070,\n",
       " '##迩': 19888,\n",
       " '##疗': 17602,\n",
       " '##黝': 21009,\n",
       " '##颁': 20619,\n",
       " '##本': 16372,\n",
       " '解': 6237,\n",
       " 'م': 270,\n",
       " '劉': 1208,\n",
       " '##稔': 17983,\n",
       " '匠': 1269,\n",
       " '##仃': 13843,\n",
       " '##ware': 10534,\n",
       " '鹞': 7910,\n",
       " '##れ': 9223,\n",
       " '##癇': 17673,\n",
       " '钢': 7167,\n",
       " '箝': 5051,\n",
       " 'fun': 9575,\n",
       " '##イト': 10516,\n",
       " '盼': 4687,\n",
       " '逆': 6847,\n",
       " '隠': 7398,\n",
       " '##俐': 13976,\n",
       " '##碱': 17879,\n",
       " '##腔': 18636,\n",
       " '##.': 13330,\n",
       " '##۩': 13441,\n",
       " '##萨': 18912,\n",
       " '##鉴': 20120,\n",
       " '慢': 2714,\n",
       " '##煨': 17270,\n",
       " '##窠': 18032,\n",
       " 'miui': 11342,\n",
       " '##睫': 17781,\n",
       " '唉': 1536,\n",
       " '##兼': 14133,\n",
       " '塊': 1846,\n",
       " '##驰': 20777,\n",
       " '##63': 9373,\n",
       " '沢': 3767,\n",
       " '##郜': 20006,\n",
       " '础': 4794,\n",
       " '键': 7241,\n",
       " '踹': 6684,\n",
       " '##张': 15533,\n",
       " '##2007': 10604,\n",
       " '##館': 20688,\n",
       " '010': 9138,\n",
       " '畸': 4535,\n",
       " '萨': 5855,\n",
       " '##蛇': 19083,\n",
       " '##间': 20370,\n",
       " '宦': 2149,\n",
       " '##韋': 20557,\n",
       " '##魯': 20855,\n",
       " '荫': 5789,\n",
       " '瑠': 4450,\n",
       " '##沫': 16830,\n",
       " '摔': 3035,\n",
       " '##削': 14238,\n",
       " '##椰': 16553,\n",
       " '腋': 5573,\n",
       " '溅': 3972,\n",
       " '闸': 7316,\n",
       " '鸳': 7892,\n",
       " '敎': 3129,\n",
       " '酷': 6999,\n",
       " '##ね': 8900,\n",
       " 'display': 11830,\n",
       " '##灏': 17176,\n",
       " 'fan': 11925,\n",
       " '券': 1171,\n",
       " '##勁': 14290,\n",
       " '##眠': 17754,\n",
       " '铲': 7211,\n",
       " '硬': 4801,\n",
       " '面': 7481,\n",
       " '##鹌': 20961,\n",
       " '##ang': 8688,\n",
       " '##饒': 20698,\n",
       " 'mon': 8556,\n",
       " '##妮': 15041,\n",
       " '##液': 16947,\n",
       " 'gm': 12277,\n",
       " '##敖': 16190,\n",
       " '##蚁': 19066,\n",
       " '##≪': 13549,\n",
       " '驍': 7707,\n",
       " 'する': 8553,\n",
       " 'demo': 10828,\n",
       " '##苫': 18794,\n",
       " 'comments': 8616,\n",
       " '##柞': 16443,\n",
       " '##轧': 19814,\n",
       " '蕨': 5938,\n",
       " 'β': 211,\n",
       " '##e': 8154,\n",
       " '##酩': 20047,\n",
       " '##飆': 20655,\n",
       " '鏖': 7123,\n",
       " '稽': 4942,\n",
       " '##per': 9063,\n",
       " '499': 9927,\n",
       " '##ᵘ': 13495,\n",
       " '##廂': 15496,\n",
       " '##咱': 14550,\n",
       " '榛': 3527,\n",
       " '##庆': 15469,\n",
       " '氢': 3705,\n",
       " '毀': 3672,\n",
       " '##rge': 10756,\n",
       " '戚': 2774,\n",
       " '##カー': 10175,\n",
       " 'kiehl': 8587,\n",
       " '##sia': 12557,\n",
       " '##ze': 9349,\n",
       " '跋': 6648,\n",
       " '##幔': 15447,\n",
       " '新': 3173,\n",
       " '僧': 1014,\n",
       " '谤': 6470,\n",
       " '##巳': 15405,\n",
       " '##记': 19438,\n",
       " '##髦': 20828,\n",
       " '##箱': 18113,\n",
       " '##鹃': 20955,\n",
       " '##報': 14898,\n",
       " 'min': 9573,\n",
       " '嵯': 2321,\n",
       " '##睏': 17768,\n",
       " '2765': 9513,\n",
       " '##恻': 15686,\n",
       " '##益': 17717,\n",
       " '##練': 18287,\n",
       " '鷗': 7875,\n",
       " '熬': 4228,\n",
       " '##well': 9665,\n",
       " '涌': 3869,\n",
       " '##囟': 14784,\n",
       " '##驥': 20772,\n",
       " '##盎': 17718,\n",
       " 'elle': 11593,\n",
       " '嘣': 1663,\n",
       " '沐': 3759,\n",
       " 'after': 10100,\n",
       " 'lake': 10556,\n",
       " '萧': 5854,\n",
       " '帛': 2368,\n",
       " '##5': 8157,\n",
       " '気': 3700,\n",
       " 'gmt': 12828,\n",
       " '52sykb': 12846,\n",
       " '##н': 13412,\n",
       " '↓↓↓': 9010,\n",
       " '##崁': 15354,\n",
       " '##毘': 16743,\n",
       " '##荒': 18831,\n",
       " '##蹉': 19746,\n",
       " '譬': 6357,\n",
       " '##譁': 19405,\n",
       " '##mbps': 12399,\n",
       " '##阜': 20395,\n",
       " '##﹚': 21065,\n",
       " '腌': 5574,\n",
       " '##怒': 15641,\n",
       " '##掛': 16022,\n",
       " '##蓑': 18959,\n",
       " '##門': 20328,\n",
       " '1966': 9093,\n",
       " '163': 8737,\n",
       " '##逮': 19923,\n",
       " '##<': 13334,\n",
       " '俄': 915,\n",
       " '##our': 9832,\n",
       " '##吹': 14487,\n",
       " '##ロク': 11416,\n",
       " '##坟': 14841,\n",
       " '壽': 1904,\n",
       " '绳': 5334,\n",
       " '##蜃': 19102,\n",
       " '呕': 1445,\n",
       " '処': 1129,\n",
       " '柳': 3394,\n",
       " '##蘋': 19038,\n",
       " '乱': 744,\n",
       " '嘯': 1669,\n",
       " '昶': 3225,\n",
       " '999': 9098,\n",
       " '18000': 12499,\n",
       " '##凋': 14175,\n",
       " '##咸': 14553,\n",
       " '湖': 3959,\n",
       " 'class': 9380,\n",
       " '##sit': 12618,\n",
       " '##缨': 18423,\n",
       " '##髏': 20822,\n",
       " 'そして': 11812,\n",
       " '006': 12526,\n",
       " '##ため': 10312,\n",
       " '砒': 4776,\n",
       " '瘘': 4600,\n",
       " '##驍': 20764,\n",
       " '関': 7283,\n",
       " '罂': 5377,\n",
       " '##虔': 19049,\n",
       " '甫': 4502,\n",
       " '眷': 4703,\n",
       " '##嘩': 14723,\n",
       " 'v4': 10942,\n",
       " '褶': 6195,\n",
       " '##倾': 14024,\n",
       " '宾': 2161,\n",
       " '##穫': 18010,\n",
       " '露': 7463,\n",
       " 'お': 540,\n",
       " '潆': 4044,\n",
       " '##粘': 18168,\n",
       " 'q': 159,\n",
       " '##压': 14384,\n",
       " '##苹': 18798,\n",
       " '##敏': 16187,\n",
       " '##外': 14969,\n",
       " '揄': 2985,\n",
       " '掲': 2977,\n",
       " '色': 5682,\n",
       " '钡': 7166,\n",
       " '333': 10745,\n",
       " '##權': 16666,\n",
       " '##經': 18252,\n",
       " '##φ': 13399,\n",
       " 'かかります': 9791,\n",
       " 'premium': 10371,\n",
       " 'たと': 12883,\n",
       " '##毒': 16738,\n",
       " '廊': 2443,\n",
       " '416': 12453,\n",
       " '##雖': 20483,\n",
       " '専': 2197,\n",
       " 'いします': 10342,\n",
       " '##匯': 14331,\n",
       " '##覺': 19278,\n",
       " '##锑': 20287,\n",
       " '##饲': 20711,\n",
       " '383': 12510,\n",
       " '獄': 4352,\n",
       " 'avira': 10473,\n",
       " '##睛': 17771,\n",
       " '殆': 3651,\n",
       " '节': 5688,\n",
       " '##圆': 14806,\n",
       " '崴': 2311,\n",
       " '拥': 2881,\n",
       " '滝': 4004,\n",
       " '稜': 4929,\n",
       " '将': 2199,\n",
       " '筛': 5033,\n",
       " 'online': 8314,\n",
       " '##ico': 10641,\n",
       " '##ead': 12943,\n",
       " '##墮': 14933,\n",
       " '##撮': 16122,\n",
       " '##谛': 19522,\n",
       " '燒': 4240,\n",
       " '##∠': 13535,\n",
       " '##瀋': 17160,\n",
       " '##勐': 14295,\n",
       " '昏': 3210,\n",
       " 'korea': 11984,\n",
       " '售': 1545,\n",
       " 'qs': 11974,\n",
       " '2400': 9851,\n",
       " '萤': 5851,\n",
       " '##玷': 17445,\n",
       " '##鳌': 20902,\n",
       " '##とても': 13211,\n",
       " '景': 3250,\n",
       " '##ょう': 11663,\n",
       " '##虾': 19064,\n",
       " '##溥': 17038,\n",
       " '42': 8239,\n",
       " '秒': 4907,\n",
       " '脉': 5549,\n",
       " '##慈': 15762,\n",
       " '##戰': 15839,\n",
       " '##愉': 15747,\n",
       " '##撇': 16107,\n",
       " '牴': 4292,\n",
       " '肚': 5496,\n",
       " '福': 4886,\n",
       " '稲': 4936,\n",
       " 'joy': 12668,\n",
       " '##焗': 17244,\n",
       " '☼': 485,\n",
       " '榖': 3526,\n",
       " 'sat': 8536,\n",
       " '淚': 3907,\n",
       " '321': 9883,\n",
       " '##驱': 20778,\n",
       " '##由': 17564,\n",
       " 'ago': 8498,\n",
       " '##貓': 19563,\n",
       " '10℃': 9115,\n",
       " '##袁': 19202,\n",
       " '##〇': 13649,\n",
       " '寮': 2185,\n",
       " '閔': 7280,\n",
       " '##去': 14400,\n",
       " '##码': 17829,\n",
       " '瑪': 4454,\n",
       " '611': 12768,\n",
       " '##稀': 17978,\n",
       " '昆': 3204,\n",
       " '官': 2135,\n",
       " '蜇': 6047,\n",
       " '道': 6887,\n",
       " '##纍': 18323,\n",
       " '椎': 3491,\n",
       " '暴': 3274,\n",
       " '蒞': 5887,\n",
       " '##找': 15880,\n",
       " '耆': 5443,\n",
       " '##捨': 16000,\n",
       " 'pc': 8295,\n",
       " '◆◆': 11146,\n",
       " '奏': 1941,\n",
       " '521': 11411,\n",
       " '耕': 5449,\n",
       " '腆': 5570,\n",
       " '##襁': 19254,\n",
       " '觑': 6234,\n",
       " '##抉': 15884,\n",
       " '##鄧': 20028,\n",
       " 'booking': 9309,\n",
       " '##诉': 19458,\n",
       " '失': 1927,\n",
       " '##浆': 16898,\n",
       " '顱': 7550,\n",
       " '躪': 6715,\n",
       " '象': 6496,\n",
       " '122': 9203,\n",
       " '##aa': 10226,\n",
       " '╯': 445,\n",
       " 'オーフン5': 11810,\n",
       " '##肱': 18565,\n",
       " '##蟬': 19156,\n",
       " '蠟': 6109,\n",
       " '##塗': 14907,\n",
       " '##錳': 20156,\n",
       " '##漩': 17090,\n",
       " 'てすか': 11244,\n",
       " '##腩': 18640,\n",
       " '靠': 7479,\n",
       " '269': 11023,\n",
       " '##辱': 19859,\n",
       " '8000': 8623,\n",
       " '##妳': 15043,\n",
       " '藕': 5969,\n",
       " '##骰': 20814,\n",
       " '##熾': 17289,\n",
       " '##do': 8828,\n",
       " '頤': 7529,\n",
       " '餉': 7620,\n",
       " '##ro': 8607,\n",
       " '蘆': 5978,\n",
       " '螃': 6083,\n",
       " '舛': 5657,\n",
       " '##繆': 18305,\n",
       " '96': 8378,\n",
       " '擒': 3084,\n",
       " '##４': 9484,\n",
       " '1936': 9481,\n",
       " '##のお': 10217,\n",
       " '##47': 9050,\n",
       " '##弊': 15521,\n",
       " '##镕': 20315,\n",
       " '蔼': 5928,\n",
       " 'this': 8554,\n",
       " 'dj': 9135,\n",
       " '序': 2415,\n",
       " '岩': 2272,\n",
       " '烷': 4180,\n",
       " '篩': 5072,\n",
       " '##ンス': 12090,\n",
       " '##侨': 13962,\n",
       " '##帶': 15437,\n",
       " 'value': 10850,\n",
       " '彅': 2490,\n",
       " '昌': 3208,\n",
       " '鋅': 7080,\n",
       " '##依': 13955,\n",
       " '##嗶': 14699,\n",
       " '##移': 17976,\n",
       " 'になります': 11028,\n",
       " '雏': 7422,\n",
       " '##厥': 14392,\n",
       " '##に': 8294,\n",
       " '灌': 4118,\n",
       " '##刑': 14209,\n",
       " '焜': 4191,\n",
       " '##架': 16430,\n",
       " '##ｅ': 10726,\n",
       " '[unused83]': 83,\n",
       " '573032185': 9970,\n",
       " 'moon': 9921,\n",
       " 'down': 10243,\n",
       " '##秣': 17967,\n",
       " '噎': 1681,\n",
       " '##揆': 16043,\n",
       " '##颧': 20647,\n",
       " '及': 1350,\n",
       " '姬': 2010,\n",
       " 'kg': 9515,\n",
       " 'os': 8721,\n",
       " '##ition': 11418,\n",
       " '##紋': 18208,\n",
       " '##瓤': 17538,\n",
       " '##释': 20082,\n",
       " '浮': 3859,\n",
       " '賭': 6551,\n",
       " '##轍': 19810,\n",
       " '##茬': 18813,\n",
       " '##喵': 14668,\n",
       " '##铃': 20247,\n",
       " '遏': 6883,\n",
       " '##雞': 20487,\n",
       " '啓': 1559,\n",
       " '##瀉': 17159,\n",
       " 'more': 8384,\n",
       " '##mb': 8517,\n",
       " '浣': 3854,\n",
       " '孳': 2117,\n",
       " '谣': 6469,\n",
       " '%': 110,\n",
       " '金': 7032,\n",
       " '1943': 9540,\n",
       " '掸': 2981,\n",
       " '##ふ': 13671,\n",
       " '##奏': 14998,\n",
       " '奄': 1935,\n",
       " '##贬': 19635,\n",
       " 'security': 11547,\n",
       " '##广': 15465,\n",
       " 'cs': 9541,\n",
       " 'ユ': 632,\n",
       " '##遢': 19953,\n",
       " '##郊': 20003,\n",
       " '##铬': 20264,\n",
       " '##靂': 20525,\n",
       " '『': 521,\n",
       " '##cca': 13144,\n",
       " '##畔': 17578,\n",
       " '##渭': 17005,\n",
       " '##晟': 16301,\n",
       " '##騏': 20755,\n",
       " '］': 8047,\n",
       " '##とう': 12471,\n",
       " 'α': 210,\n",
       " '磁': 4828,\n",
       " '撰': 3066,\n",
       " '288': 10332,\n",
       " '##蠻': 19173,\n",
       " '##値': 14014,\n",
       " '##檗': 16651,\n",
       " '麩': 7932,\n",
       " '##盅': 17714,\n",
       " '仗': 801,\n",
       " '##虹': 19061,\n",
       " '孃': 2093,\n",
       " '烟': 4170,\n",
       " '搽': 3026,\n",
       " '##徇': 15579,\n",
       " '##莪': 18867,\n",
       " '併': 882,\n",
       " '213': 10431,\n",
       " '椿': 3499,\n",
       " '盹': 4686,\n",
       " 'س': 267,\n",
       " '擦': 3092,\n",
       " '胡': 5529,\n",
       " '470': 10658,\n",
       " 'chocolate': 12474,\n",
       " '##吠': 14470,\n",
       " '勛': 1244,\n",
       " '瀨': 4114,\n",
       " '缢': 5363,\n",
       " 'light': 10310,\n",
       " 'amg': 13100,\n",
       " '##殡': 16718,\n",
       " '渍': 3931,\n",
       " '间': 7313,\n",
       " '﹚': 8008,\n",
       " '##储': 14053,\n",
       " '##瓶': 17543,\n",
       " '謔': 6338,\n",
       " '##赝': 19670,\n",
       " '躯': 6718,\n",
       " 'windows7': 12258,\n",
       " '呈': 1439,\n",
       " '秋': 4904,\n",
       " '斧': 3167,\n",
       " '##═': 13586,\n",
       " '##阖': 20392,\n",
       " '位': 855,\n",
       " '##旷': 16256,\n",
       " '##丟': 13751,\n",
       " '##御': 15596,\n",
       " '韻': 7512,\n",
       " 'くたさい': 9052,\n",
       " 'aws': 12014,\n",
       " '##君': 14466,\n",
       " '##鄲': 20032,\n",
       " '##錶': 20157,\n",
       " '語': 6295,\n",
       " 'aspx': 12363,\n",
       " '棣': 3479,\n",
       " '##傲': 14057,\n",
       " '资': 6598,\n",
       " '颯': 7592,\n",
       " '##佢': 13930,\n",
       " '1899': 11886,\n",
       " '戮': 2781,\n",
       " '##唆': 14591,\n",
       " '004': 12329,\n",
       " '囚': 1723,\n",
       " '狠': 4321,\n",
       " '晩': 3248,\n",
       " 'す': 548,\n",
       " '贼': 6592,\n",
       " '##ましょう': 10759,\n",
       " '##ф': 13419,\n",
       " 'u': 163,\n",
       " '沦': 3770,\n",
       " '##lly': 9456,\n",
       " 'charlie': 12962,\n",
       " '##钓': 20214,\n",
       " '∩': 386,\n",
       " '##てきる': 11415,\n",
       " '假': 969,\n",
       " '慫': 2718,\n",
       " '羅': 5397,\n",
       " '鳶': 7856,\n",
       " '谟': 6467,\n",
       " '##槿': 16611,\n",
       " '415': 12114,\n",
       " 'member': 11120,\n",
       " '##撤': 16116,\n",
       " '咬': 1490,\n",
       " 'f16': 12799,\n",
       " '媒': 2054,\n",
       " 'て': 555,\n",
       " '惚': 2666,\n",
       " '68': 8360,\n",
       " 'からお': 9669,\n",
       " '##ります': 11395,\n",
       " 'dl': 11961,\n",
       " '驗': 7710,\n",
       " '##75': 9183,\n",
       " 'allen': 11736,\n",
       " '##ψ': 13401,\n",
       " '##玻': 17447,\n",
       " '##诰': 19486,\n",
       " '−': 377,\n",
       " '##迦': 19887,\n",
       " '陡': 7367,\n",
       " '辞': 6791,\n",
       " 'pe': 9205,\n",
       " '##oud': 12867,\n",
       " '##当': 15553,\n",
       " '##蹲': 19759,\n",
       " '##right': 11264,\n",
       " '蜊': 6049,\n",
       " '293': 11855,\n",
       " 'il': 12197,\n",
       " '##赠': 19672,\n",
       " '劵': 1229,\n",
       " '讨': 6374,\n",
       " 'b1': 9338,\n",
       " 'ｸ': 8089,\n",
       " '##遍': 19938,\n",
       " '##ｽ': 21113,\n",
       " '鮪': 7802,\n",
       " '##躬': 19774,\n",
       " 'iphone': 8210,\n",
       " '║': 439,\n",
       " '梭': 3460,\n",
       " '竜': 4992,\n",
       " '騏': 7698,\n",
       " '簇': 5077,\n",
       " 'carol': 9196,\n",
       " '07': 8155,\n",
       " '##lor': 12422,\n",
       " 'ᄒ': 303,\n",
       " '窦': 4977,\n",
       " '##last': 13177,\n",
       " '##踌': 19728,\n",
       " 'ᄀ': 288,\n",
       " '長': 7269,\n",
       " 'される': 12033,\n",
       " '##戊': 15820,\n",
       " '创': 1158,\n",
       " '##蹴': 19760,\n",
       " '稣': 4933,\n",
       " '##沓': 16818,\n",
       " '##辣': 19850,\n",
       " '瀕': 4106,\n",
       " '告': 1440,\n",
       " '##颛': 20638,\n",
       " '##槲': 16607,\n",
       " '锑': 7230,\n",
       " '##傾': 14062,\n",
       " '##稠': 17989,\n",
       " '疟': 4549,\n",
       " '則': 1179,\n",
       " '岑': 2263,\n",
       " '##枰': 16427,\n",
       " '##猴': 17404,\n",
       " '諷': 6327,\n",
       " '蜱': 6061,\n",
       " '##∥': 10458,\n",
       " '##ない': 8560,\n",
       " '張': 2484,\n",
       " '##琬': 17485,\n",
       " '##踩': 19735,\n",
       " '##鼐': 21017,\n",
       " '夔': 1910,\n",
       " '##埵': 14878,\n",
       " '##岂': 15317,\n",
       " '##漾': 17099,\n",
       " '備': 991,\n",
       " '帖': 2365,\n",
       " '艳': 5683,\n",
       " '侨': 905,\n",
       " '报': 2845,\n",
       " '霾': 7467,\n",
       " 'jerry': 13111,\n",
       " '[unused12]': 12,\n",
       " '勺': 1254,\n",
       " 'van': 9933,\n",
       " 'oh': 9941,\n",
       " '##膾': 18673,\n",
       " '##邸': 19997,\n",
       " '溝': 3978,\n",
       " 'bang': 12067,\n",
       " 'into': 12609,\n",
       " '泠': 3795,\n",
       " '##传': 13894,\n",
       " '##蟲': 19157,\n",
       " '絕': 5179,\n",
       " 'ⅰ': 363,\n",
       " '礪': 4847,\n",
       " '褐': 6186,\n",
       " '##個': 14000,\n",
       " '##銃': 20123,\n",
       " '##顱': 20607,\n",
       " '挽': 2924,\n",
       " '##傢': 14050,\n",
       " '捋': 2930,\n",
       " '##ᵉ': 13490,\n",
       " '##富': 15225,\n",
       " '##從': 15594,\n",
       " '砥': 4783,\n",
       " 'ワ': 640,\n",
       " '廿': 2457,\n",
       " '##one': 9021,\n",
       " '##寰': 15243,\n",
       " '##氟': 16760,\n",
       " '##终': 18360,\n",
       " '##苷': 18797,\n",
       " '睁': 4709,\n",
       " '詛': 6269,\n",
       " 'twd600': 11230,\n",
       " '##嘛': 14715,\n",
       " 'read': 8649,\n",
       " '##hg': 12207,\n",
       " '##祥': 17929,\n",
       " 'not': 9059,\n",
       " '艙': 5676,\n",
       " '鹘': 7908,\n",
       " '##辜': 19847,\n",
       " '鲷': 7840,\n",
       " '##炅': 17196,\n",
       " '殇': 3652,\n",
       " 'セ': 606,\n",
       " '##屁': 15287,\n",
       " 'plurk': 10498,\n",
       " '##詳': 19341,\n",
       " '##筆': 18079,\n",
       " '##哔': 14573,\n",
       " '辭': 6798,\n",
       " '##✿': 13640,\n",
       " '倍': 945,\n",
       " '紐': 5153,\n",
       " '丧': 700,\n",
       " 'toyota': 10632,\n",
       " '##►': 13612,\n",
       " '炯': 4153,\n",
       " '龄': 7977,\n",
       " '##xi': 10201,\n",
       " '##牵': 17350,\n",
       " '袄': 6147,\n",
       " '賺': 6553,\n",
       " '##是': 16278,\n",
       " '##礁': 17899,\n",
       " '##详': 19479,\n",
       " '诱': 6430,\n",
       " '485': 11763,\n",
       " '##獺': 17424,\n",
       " '##erry': 11329,\n",
       " '～～': 8734,\n",
       " '##《': 13652,\n",
       " '##仍': 13850,\n",
       " '##rn': 9256,\n",
       " '##亟': 13823,\n",
       " '##做': 14033,\n",
       " '##mhz': 9932,\n",
       " '##ses': 12856,\n",
       " '求': 3724,\n",
       " 'ctrip': 8223,\n",
       " '259': 10987,\n",
       " '##右': 14438,\n",
       " '笠': 5015,\n",
       " '##淹': 16979,\n",
       " '##私': 17957,\n",
       " '佳': 881,\n",
       " 'engine': 11764,\n",
       " '唄': 1533,\n",
       " '##暗': 16323,\n",
       " '╞': 441,\n",
       " '硐': 4796,\n",
       " '##りの': 12407,\n",
       " '耍': 5446,\n",
       " '##✪': 13638,\n",
       " '##伯': 13900,\n",
       " '淆': 3898,\n",
       " 'eds': 12191,\n",
       " '##閏': 20333,\n",
       " '##涟': 16935,\n",
       " '##茴': 18818,\n",
       " '粗': 5110,\n",
       " '##铢': 20259,\n",
       " '⊙': 400,\n",
       " '##囪': 14791,\n",
       " 'aa': 9563,\n",
       " '被': 6158,\n",
       " '##涩': 16943,\n",
       " '##狞': 17377,\n",
       " '##κ': 13388,\n",
       " '##觸': 19297,\n",
       " '##馥': 20734,\n",
       " '栩': 3414,\n",
       " '##lab': 11532,\n",
       " '强': 2487,\n",
       " '旖': 3185,\n",
       " '353': 12115,\n",
       " '##25': 8743,\n",
       " '##啞': 14620,\n",
       " '##∼': 13540,\n",
       " '##郷': 20018,\n",
       " '慘': 2711,\n",
       " '##┃': 13579,\n",
       " '##甄': 17545,\n",
       " '访': 6393,\n",
       " '##聂': 18519,\n",
       " '##捱': 16003,\n",
       " '減': 3938,\n",
       " 'sputniknews': 11376,\n",
       " 'shell': 10651,\n",
       " '##毗': 16742,\n",
       " '嚓': 1706,\n",
       " '##无': 16244,\n",
       " '##盯': 17738,\n",
       " 'vornado': 13036,\n",
       " '##镗': 20317,\n",
       " '##闻': 20376,\n",
       " '峦': 2287,\n",
       " '巽': 2352,\n",
       " '怡': 2592,\n",
       " '隈': 7385,\n",
       " '威': 2014,\n",
       " '△': 465,\n",
       " '##杀': 16381,\n",
       " '##祕': 17918,\n",
       " '##tax': 13155,\n",
       " '##code': 10802,\n",
       " '##娄': 15073,\n",
       " '禧': 4889,\n",
       " '臘': 5626,\n",
       " '##跤': 19715,\n",
       " 'publishing': 13306,\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1632c11f-5646-443e-89fc-cfc323803f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize:['我', '有', '一', '個', '夢', '，', 'i', 'have', 'an', 'inc', '##red', '##ible', 'dream', '!']\n",
      "convert_tokens_to_ids:[2769, 3300, 671, 943, 1918, 8024, 151, 9531, 9064, 8910, 9749, 12413, 10252, 106]\n",
      "encode (tokenize+convert_tokens_to_ids):[2769, 3300, 671, 943, 1918, 8024, 151, 9531, 9064, 8910, 9749, 12413, 10252, 106]\n",
      "convert_ids_to_tokens:['我', '有', '一', '個', '夢', '，', 'i', 'have', 'an', 'inc', '##red', '##ible', 'dream', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"我有一個夢，I have an incredible dream!\")\n",
    "print(f\"tokenize:{tokens}\")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"convert_tokens_to_ids:{ids}\")\n",
    "\n",
    "ids2 = tokenizer.encode(\"我有一個夢，I have an incredible dream!\", add_special_tokens=False)\n",
    "print(f\"encode (tokenize+convert_tokens_to_ids):{ids2}\")\n",
    "tokens2 = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(f\"convert_ids_to_tokens:{tokens2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867b1aa-a7e0-4513-a78c-5ebd30585d5c",
   "metadata": {},
   "source": [
    "### tokenizer.__call__()\n",
    "`input_ids`\n",
    "`attention_mask`\n",
    "`token_type_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "782ae379-6094-4288-b88d-8776dae58313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2769,  3300,   671,   943,  1918,  8024,   151,  9531,  9064,\n",
       "          8910,  9749, 12413, 10252,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"我有一個夢，I have an incredible dream!\", add_special_tokens=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1819c9d-ca8a-44b8-b0dc-7cb516199471",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bfda67-d44a-4f50-afeb-ab25cd6fd903",
   "metadata": {},
   "source": [
    "### 不帶Model Head的模型\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained    \n",
    "這邊列出具體支援那些Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9858bf15-2cdf-45b3-b4c9-76c13773dabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.5823,  0.0160,  0.0598,  ...,  0.4973, -0.0854, -0.2723],\n",
       "         [ 0.7501, -0.2440,  0.2002,  ..., -0.0857, -0.2035, -0.2993],\n",
       "         [ 0.5779,  0.0498, -0.3363,  ..., -0.4652,  0.0127,  0.2393],\n",
       "         ...,\n",
       "         [ 0.0386,  0.5116,  0.2850,  ...,  0.0659, -0.0661, -0.2211],\n",
       "         [ 0.5911, -0.0726,  0.1620,  ..., -0.6148, -0.2701, -0.2920],\n",
       "         [ 0.5772,  0.0154,  0.0563,  ...,  0.4985, -0.0868, -0.2694]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-3.2027e-01, -9.9989e-01, -9.9936e-01, -6.1967e-01,  9.9206e-01,\n",
       "          2.6552e-01, -2.0064e-01,  5.6108e-03,  9.5703e-01,  9.9709e-01,\n",
       "         -4.2273e-02, -9.9999e-01, -2.1317e-01,  9.9944e-01, -9.9998e-01,\n",
       "          9.9814e-01,  9.9397e-01,  6.5082e-01, -9.8280e-01, -1.2920e-01,\n",
       "         -9.1872e-01, -6.6236e-01,  2.2591e-01,  9.4480e-01,  9.8757e-01,\n",
       "         -9.8333e-01, -9.9996e-01,  8.3699e-02, -9.3286e-01, -9.9903e-01,\n",
       "         -9.2296e-01, -9.9910e-01,  6.5642e-02,  1.2290e-01,  9.8371e-01,\n",
       "         -9.6065e-01,  2.8697e-01, -7.7186e-01, -9.7982e-01, -9.8467e-01,\n",
       "         -1.0599e-01,  9.6414e-01, -8.7592e-02,  9.9895e-01,  3.4180e-01,\n",
       "          3.3159e-01,  9.9993e-01,  9.9522e-01, -1.0721e-01, -1.3832e-01,\n",
       "         -1.5820e-01, -1.8800e-01, -8.8516e-01,  9.9362e-01, -8.6468e-02,\n",
       "          3.0359e-01,  9.9853e-01, -9.9981e-01, -9.9806e-01,  9.9334e-01,\n",
       "         -9.8669e-01,  9.8981e-01,  9.9868e-01,  6.6878e-01, -9.2440e-01,\n",
       "          9.9924e-01,  9.7073e-01,  6.8414e-01, -5.2019e-01, -9.9993e-01,\n",
       "         -2.4237e-01, -9.7829e-01, -9.9215e-01, -2.2464e-01, -7.0803e-02,\n",
       "         -9.9313e-01,  9.9866e-01, -2.0748e-02,  9.9948e-01,  5.1921e-02,\n",
       "         -9.8972e-01, -7.1019e-02, -4.4037e-02,  2.0582e-02,  9.7938e-01,\n",
       "          9.9998e-01,  1.6996e-01, -9.8386e-01, -2.3646e-01, -9.9227e-01,\n",
       "          1.5295e-01,  9.9903e-01,  9.9999e-01, -9.9957e-01,  9.9839e-01,\n",
       "         -4.2613e-01,  2.8414e-01, -3.2309e-01, -9.9193e-01,  9.6040e-01,\n",
       "         -6.2105e-01,  2.6096e-02,  9.9992e-01,  9.7771e-01,  2.6429e-01,\n",
       "         -9.9936e-01, -7.7041e-01,  9.9271e-01, -9.9660e-01,  1.6977e-01,\n",
       "          9.9996e-01,  5.2334e-01,  9.9992e-01,  9.9958e-01,  9.9754e-01,\n",
       "         -9.9654e-01, -2.8294e-01,  1.9297e-01, -9.9923e-01,  8.9707e-01,\n",
       "         -9.7030e-01,  9.6220e-01, -8.3972e-01,  2.0418e-01, -1.2028e-02,\n",
       "         -9.9836e-01,  2.6371e-01,  1.0005e-01, -8.3852e-01, -9.9217e-01,\n",
       "         -9.9860e-01, -9.9986e-01,  5.9897e-01,  8.1361e-01, -8.2791e-03,\n",
       "         -5.7063e-01, -3.5030e-02, -2.8565e-01, -9.9947e-01, -9.9944e-01,\n",
       "         -9.9992e-01, -7.2797e-01, -7.9724e-01,  9.9810e-01, -9.9895e-01,\n",
       "          9.9887e-01, -9.8973e-01,  9.9837e-01,  8.8466e-01, -1.6343e-02,\n",
       "          1.4590e-01,  6.9389e-02, -9.7652e-01,  1.5525e-01, -1.6358e-01,\n",
       "          9.9525e-01,  9.4537e-01,  7.9475e-01, -9.9751e-01,  9.9973e-01,\n",
       "         -5.6291e-01,  9.8769e-01, -8.7136e-03,  9.5970e-01,  1.0000e+00,\n",
       "         -9.9646e-01,  1.0206e-01, -9.9998e-01,  8.0384e-02, -3.3810e-02,\n",
       "          9.9800e-01,  9.9900e-01,  5.1674e-01,  9.8575e-01,  3.2297e-01,\n",
       "         -9.9854e-01, -9.5266e-01, -9.9585e-01,  6.8886e-01,  9.9930e-01,\n",
       "          1.8088e-02,  8.9090e-01,  1.0000e+00, -6.9833e-01,  9.0133e-01,\n",
       "          2.3036e-02, -3.4757e-01, -9.3374e-01,  2.5077e-01, -2.4980e-01,\n",
       "          9.8906e-01, -9.8584e-01,  1.3588e-01,  6.5411e-01,  1.1880e-01,\n",
       "         -8.1344e-02, -9.7864e-01, -9.9489e-01,  9.9986e-01,  9.9729e-01,\n",
       "         -3.0760e-02, -2.6430e-01,  9.9929e-01, -1.8043e-01,  9.9922e-01,\n",
       "         -1.0597e-02,  8.5417e-01, -3.6546e-01,  9.8961e-01,  2.2596e-01,\n",
       "          3.1145e-01, -8.9023e-02,  9.9563e-01,  3.3753e-01, -9.8525e-01,\n",
       "         -1.1195e-01, -1.2915e-01,  3.0066e-01, -9.1589e-01, -9.1678e-02,\n",
       "         -3.0654e-01,  9.9703e-01, -7.6407e-02, -9.8011e-01,  9.8439e-01,\n",
       "         -9.7865e-01,  7.0636e-01, -9.9994e-01, -9.7559e-01,  9.9974e-01,\n",
       "         -4.1920e-01, -9.9998e-01,  4.0160e-01,  9.9998e-01, -8.9946e-01,\n",
       "          9.9961e-01, -3.0541e-01, -9.9932e-01, -1.8615e-01,  5.0013e-02,\n",
       "         -9.9999e-01, -9.9770e-01, -1.0000e+00,  1.9027e-01, -1.2022e-01,\n",
       "          9.8409e-01, -9.9998e-01,  7.4517e-02, -9.9997e-01,  9.9185e-01,\n",
       "          9.9959e-01,  9.7727e-02, -2.4812e-01,  9.9984e-01,  8.9569e-01,\n",
       "          8.7096e-02,  3.4038e-01,  5.9535e-01, -1.1334e-01,  9.2170e-01,\n",
       "          7.6877e-02,  9.9861e-01, -9.9966e-01,  9.5678e-02, -9.1478e-01,\n",
       "         -9.9882e-01, -2.8550e-01, -2.2523e-01,  4.4285e-02,  2.7553e-02,\n",
       "         -1.6126e-01,  2.0648e-01,  9.7318e-01,  9.6926e-01,  9.9999e-01,\n",
       "          9.8397e-01,  9.9989e-01,  9.9997e-01, -2.8305e-01, -8.2481e-01,\n",
       "         -7.1796e-01, -1.8093e-01, -9.9968e-01, -9.6833e-01, -9.8720e-01,\n",
       "          9.1279e-01,  4.3686e-01,  1.0000e+00, -9.9998e-01,  9.9995e-01,\n",
       "         -8.6238e-01,  5.7731e-02, -1.2871e-01,  1.2960e-02, -5.9737e-01,\n",
       "         -2.0045e-01,  9.5869e-01,  5.0260e-01,  8.2679e-02,  9.5858e-01,\n",
       "         -7.9778e-02, -1.5962e-01,  3.8746e-01, -2.4309e-01,  9.9380e-01,\n",
       "         -5.2330e-01,  9.7849e-01,  3.7336e-02,  9.7247e-01, -1.3785e-01,\n",
       "         -9.9987e-01,  9.9464e-01, -9.8901e-01, -3.6076e-01, -9.9903e-01,\n",
       "         -9.1555e-01,  3.1858e-01, -9.9602e-01,  9.4984e-01,  9.7972e-01,\n",
       "         -9.3849e-02,  2.4032e-01, -9.9949e-01, -7.5362e-01,  9.8987e-01,\n",
       "          9.4313e-01, -1.0000e+00,  9.6154e-01,  7.2191e-01, -7.7190e-01,\n",
       "          3.6661e-01,  8.3478e-01, -9.6190e-01,  9.9992e-01, -9.9893e-01,\n",
       "          6.1343e-02,  9.0798e-01,  6.2910e-02, -9.9955e-01, -7.1231e-01,\n",
       "         -6.7057e-02,  6.5118e-01,  1.2052e-01,  9.9924e-01, -4.2051e-01,\n",
       "         -6.6394e-02, -9.9931e-01, -9.2505e-01, -9.0393e-02,  1.6424e-01,\n",
       "          9.9890e-01, -9.9971e-01, -8.8230e-01,  9.9590e-01, -9.9602e-01,\n",
       "         -1.3748e-01,  2.9016e-01, -2.8823e-01,  3.7290e-01, -9.8336e-01,\n",
       "         -9.9554e-01, -9.9215e-01,  9.9598e-01, -1.2756e-01, -3.2322e-02,\n",
       "          9.6711e-01,  9.9949e-01,  9.9888e-01, -9.8822e-01,  1.4713e-01,\n",
       "          9.9739e-01, -1.2175e-01, -4.8940e-02, -9.4108e-01, -2.8848e-01,\n",
       "         -9.8765e-01, -8.1921e-01,  7.7159e-02,  6.1056e-01,  4.8323e-01,\n",
       "         -9.9427e-01,  9.9976e-01,  9.9320e-01,  9.9998e-01, -7.4947e-02,\n",
       "         -9.4726e-01,  5.9524e-01, -9.3995e-01, -9.9923e-01, -1.4089e-02,\n",
       "          9.9851e-01, -9.9895e-01,  2.7486e-01,  8.7061e-01, -9.9639e-01,\n",
       "          9.9871e-01, -7.6592e-01,  1.9393e-01, -1.0000e+00, -9.8991e-01,\n",
       "         -1.0000e+00,  9.9786e-01,  1.9239e-01, -1.5207e-01, -9.6984e-01,\n",
       "          9.9988e-01,  9.0242e-01, -6.9850e-01, -1.8125e-01,  9.9913e-01,\n",
       "         -4.1461e-01,  6.5278e-01, -1.0000e+00, -9.3389e-01,  9.0304e-01,\n",
       "         -1.3008e-01,  9.9886e-01, -5.8006e-01, -9.9611e-01,  9.2924e-01,\n",
       "          9.9636e-01,  2.7422e-01, -9.8810e-01, -9.3708e-01,  3.4710e-01,\n",
       "          4.3521e-01,  1.7780e-02, -4.3560e-01, -2.4731e-01,  9.8626e-01,\n",
       "         -3.0870e-01,  5.1629e-02,  3.1114e-02,  9.9940e-01,  9.0968e-01,\n",
       "         -9.9327e-01, -5.6571e-01,  4.0166e-03, -8.7058e-01, -9.8948e-01,\n",
       "         -9.8974e-01, -1.3303e-01,  4.1247e-02,  2.6000e-01, -8.7103e-01,\n",
       "         -9.9986e-01, -9.6495e-01,  1.3123e-01, -9.8979e-01, -9.6093e-01,\n",
       "          1.3020e-01, -9.9988e-01, -9.9145e-01,  9.9741e-01, -9.9880e-01,\n",
       "          6.4099e-02,  9.8003e-01,  9.5446e-01, -9.9998e-01, -2.5016e-02,\n",
       "          9.9456e-01, -9.7786e-01,  1.5986e-01, -9.8188e-01,  2.5493e-01,\n",
       "         -6.1706e-01, -9.9960e-01,  1.0993e-01,  9.9945e-01,  9.9884e-01,\n",
       "          9.9780e-01,  9.3174e-01, -7.3764e-01,  9.7701e-01,  9.9010e-01,\n",
       "          9.9911e-01, -1.6132e-01, -1.9735e-01, -9.9995e-01,  7.6265e-01,\n",
       "          8.6831e-01,  1.5380e-01,  1.5297e-01, -9.9635e-01,  1.5892e-01,\n",
       "         -6.5829e-01,  4.7538e-01,  1.0000e+00,  8.6534e-01,  6.9581e-01,\n",
       "         -9.9997e-01,  5.1027e-01, -4.6230e-01, -1.6662e-01, -9.1782e-01,\n",
       "          2.4880e-01,  9.9991e-01, -9.9902e-01,  6.0414e-01, -9.8093e-01,\n",
       "         -9.9333e-01,  9.9994e-01, -9.9994e-01,  9.9921e-01,  9.3137e-01,\n",
       "         -7.2111e-01, -3.9224e-02, -3.0468e-01,  2.2809e-01,  1.5660e-03,\n",
       "         -1.1117e-01,  5.2867e-01, -5.6632e-02, -9.9903e-01, -3.2377e-03,\n",
       "          9.6810e-01, -4.8180e-02, -2.2215e-01, -9.9846e-01,  7.8062e-02,\n",
       "          9.9778e-01, -9.9067e-01, -9.9984e-01, -2.6074e-01, -2.5977e-01,\n",
       "          1.4454e-01, -6.5701e-01,  6.3586e-02, -1.7024e-01, -9.9351e-01,\n",
       "         -4.5688e-02,  9.9016e-01,  3.0242e-01,  7.4830e-01, -8.9732e-01,\n",
       "         -9.7377e-01, -4.2641e-01,  9.9506e-01,  9.7449e-01, -9.9825e-01,\n",
       "         -9.9790e-01,  6.1269e-01,  3.3067e-01,  5.8204e-01,  9.9115e-01,\n",
       "         -4.8338e-02, -9.5311e-01,  4.7635e-02,  1.1773e-01, -3.4835e-01,\n",
       "         -7.5618e-01,  9.9999e-01, -9.7999e-01,  1.0000e+00, -9.9996e-01,\n",
       "         -9.7701e-01,  8.1965e-02,  9.9995e-01, -9.9984e-01, -1.1159e-01,\n",
       "          9.9510e-01, -9.9991e-01, -1.9131e-01, -1.4265e-01,  7.3154e-01,\n",
       "         -1.9431e-01, -1.1059e-01,  9.8413e-01, -9.8301e-01,  8.3816e-02,\n",
       "         -9.7563e-01,  5.7600e-01,  9.8371e-01, -9.9573e-01, -6.9999e-01,\n",
       "         -1.0000e+00, -2.4496e-02,  6.3013e-02, -9.9997e-01,  9.8601e-01,\n",
       "          9.9956e-01, -9.6685e-02,  6.1238e-01, -9.8264e-01, -1.4101e-01,\n",
       "         -4.8698e-02, -9.7991e-01,  3.0488e-01, -9.9833e-01, -9.1254e-02,\n",
       "         -9.8787e-01,  9.9694e-01, -9.9424e-01,  9.9640e-01,  6.9323e-01,\n",
       "         -3.9401e-01, -8.2915e-01,  3.9241e-02, -3.8585e-01, -9.9857e-01,\n",
       "          8.7419e-03, -9.9747e-01, -9.9329e-01, -5.1434e-04,  9.9854e-01,\n",
       "          9.1648e-01,  2.5183e-01,  9.9101e-01, -9.9400e-01,  1.2784e-01,\n",
       "          3.4111e-02,  9.2328e-01,  1.0000e+00, -9.9988e-01, -9.9827e-01,\n",
       "          9.9423e-01, -9.9926e-01, -1.6728e-01,  1.0000e+00, -5.5246e-01,\n",
       "          9.9988e-01,  1.6524e-01, -9.5841e-01,  1.7859e-01,  1.0638e-01,\n",
       "          9.5778e-01,  8.0534e-02,  1.8866e-01,  9.1613e-01,  1.5194e-01,\n",
       "          1.3697e-01, -9.4875e-01,  8.9554e-01,  1.7352e-01,  1.1514e-01,\n",
       "          9.9363e-01, -9.8412e-01, -9.9114e-01, -9.7263e-01,  1.2979e-01,\n",
       "         -5.5059e-01, -5.3327e-01, -2.7496e-01,  9.6417e-01,  9.9939e-01,\n",
       "         -9.9425e-01,  9.7944e-01, -1.0000e+00, -9.9993e-01,  7.4946e-02,\n",
       "          3.2606e-02,  9.7734e-01,  4.3712e-02, -9.0504e-01,  2.6802e-01,\n",
       "         -9.8935e-01,  7.3000e-01, -9.6868e-01,  9.6742e-01, -2.0909e-02,\n",
       "         -1.9474e-02,  9.9943e-01,  9.9884e-01, -9.8089e-02, -9.6947e-01,\n",
       "         -9.7845e-01,  2.8331e-01, -9.8156e-01,  9.9380e-01, -3.5520e-02,\n",
       "         -2.7201e-01,  3.8566e-02,  1.6628e-01, -9.4520e-01, -9.9949e-01,\n",
       "         -1.8391e-02,  9.4519e-01, -9.8534e-01,  8.7094e-01, -9.7712e-01,\n",
       "          9.6425e-01,  9.5619e-01,  1.0000e+00,  4.7192e-03,  9.5259e-01,\n",
       "         -9.9407e-01, -9.9832e-01,  9.9217e-01,  9.9546e-01,  9.9999e-01,\n",
       "          9.3241e-01,  7.4335e-01,  2.7431e-01, -9.9988e-01,  9.7910e-01,\n",
       "          2.3451e-01, -2.2976e-01, -1.4142e-01, -9.3717e-01, -9.9977e-01,\n",
       "          9.9759e-01, -1.0000e+00, -9.9588e-01, -9.8045e-01, -9.9201e-01,\n",
       "          8.7305e-01,  9.5942e-01,  9.7050e-01,  1.8510e-01, -9.9620e-01,\n",
       "         -9.5283e-01, -3.3486e-02, -9.7811e-01, -9.8016e-01,  5.5268e-02,\n",
       "         -1.0000e+00,  2.3361e-01,  1.4932e-01, -9.5324e-01,  2.8525e-01,\n",
       "         -7.3969e-01,  3.3970e-01,  9.8057e-01, -7.1003e-01,  5.9772e-01,\n",
       "         -7.3634e-01, -9.9883e-01,  1.3077e-01, -1.0000e+00,  9.5788e-01,\n",
       "          9.9721e-01,  1.0545e-01,  8.8941e-01, -8.0610e-01,  1.1164e-01,\n",
       "         -9.9993e-01, -9.9998e-01,  9.7339e-01,  9.9985e-01, -2.5179e-02,\n",
       "         -9.9123e-01, -2.0303e-01, -9.9778e-01, -7.7956e-02,  2.7228e-01,\n",
       "          9.9240e-01, -9.9936e-01,  9.7809e-01, -7.8693e-01,  1.5621e-01,\n",
       "          9.9378e-01, -1.0000e+00,  7.0544e-01, -9.9917e-01,  9.9864e-01,\n",
       "         -9.9997e-01,  9.9961e-01, -6.8114e-01,  2.4948e-01, -5.9653e-02,\n",
       "          8.5751e-01, -9.9979e-01, -3.3551e-01,  8.9420e-01,  9.6505e-01,\n",
       "         -3.3090e-02,  8.8106e-01, -1.9436e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "inputs = tokenizer(\"我有一個夢，I have an incredible dream!\", return_tensors=\"pt\") # 記得一定要 return_tensors=\"pt\"\n",
    "model = AutoModel.from_pretrained(\"hfl/rbt3\")\n",
    "output = model(**inputs)\n",
    "output # BaseModelOutputWithPoolingAndCrossAttentions, 回傳每個字被model分析過後的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbebe42-a833-4a9a-91e3-aab4d699291a",
   "metadata": {},
   "source": [
    "### 帶Model Head的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03c7111-bb59-4c11-829a-1ab13cdbc82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./01-Getting Started/04-model/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0606,  0.2734, -0.0070,  0.1932, -0.1230, -0.1013, -0.3791, -0.2660,\n",
       "          0.6980,  0.0691]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "clz_model = AutoModelForSequenceClassification.from_pretrained(\"./01-Getting Started/04-model/rbt3\", num_labels=10) \n",
    "# BertForSequenceClassification, , 被額外加上dropout跟linear layer等做分類\n",
    "clz_model(**inputs) # SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f71d3c-4fa8-489e-9d2d-1c453518a583",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c95f04-0f08-4a07-8457-7f61bdd7268c",
   "metadata": {},
   "source": [
    "### Torch + Pandas  \n",
    "1. 使用panda讀入檔案\n",
    "2. 使用pytorch Dataset包起來/分割\n",
    "3. 使用pytorch Dataloader (shuffle+batch+collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49f9bf2-b664-42c9-bb70-7a32bd4f8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(\"./01-Getting Started/05-datasets/ChnSentiCorp_htl_all.csv\")\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.iloc[index][\"review\"], self.data.iloc[index][\"label\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def createSplit(self, lengths=[0.9, 0.1]):\n",
    "        # split\n",
    "        from torch.utils.data import random_split\n",
    "        trainset, validset = random_split(dataset, lengths)\n",
    "        return trainset, validset\n",
    "\n",
    "dataset = MyDataset()\n",
    "trainset, validset = dataset.createSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f22cde0-1e67-4e0c-94ef-49f26569d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loader\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "def collate_func(batch):\n",
    "    # [\n",
    "    #  ('总体感觉不错。不足之处：客房分散在各个小楼里，颇有不便。前台速度慢。', 1), \n",
    "    #  .........\n",
    "    #  ('卫生设施太差，毛巾又黑又硬，下水道堵塞，床垫不好，睡上去不是很舒服。觉得服务态度还可以。对面就是长途汽车站，出租车蛮多的，出行还是很方便。', 0)\n",
    "    # ]\n",
    "    texts, labels = [], []\n",
    "    for item in batch:\n",
    "        texts.append(item[0]) # '很不错的酒店，强烈推荐。为什么要20个字'\n",
    "        labels.append(item[1]) # 1\n",
    "    inputs = tokenizer(texts, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\") # <-重點: 一次處理多筆可以增加效能\n",
    "    # 'input_ids', 'token_type_ids', 'attention_mask'\n",
    "    inputs[\"labels\"] = torch.tensor(labels) \n",
    "    return inputs\n",
    "\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate_func)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False, collate_fn=collate_func)\n",
    "next_input = next(iter(validloader))\n",
    "next_input.keys()\n",
    "# 'input_ids', 'token_type_ids', 'attention_mask',  +'labels'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b03368-c0ff-4054-a8dc-d5c32f1929e0",
   "metadata": {},
   "source": [
    "### Dataset (Huggingface Transformer style)\n",
    "https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40c90e04-3a5d-426f-87ba-63df5d13ddcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "dataset = load_dataset(\"csv\", data_files=\"./01-Getting Started/05-datasets/ChnSentiCorp_htl_all.csv\"\n",
    "                       , split='train') \n",
    "# or datasets = load_dataset(\"name\")\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c09a4385-9ace-4f67-b81b-2e66e4009317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = dataset.train_test_split(test_size=0.1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "886125b2-8458-49cd-b26f-785e093fbf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b529656fdd7041c8ab1cae2b84b00c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf0dda0c097470b9415c02c51c179fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/777 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_function(examples):\n",
    "    # 跟之前的差別:不需要padding跟return_tensors=\"pt\" (都給DataCollatorWithPadding處理)\n",
    "    tokenized_examples = tokenizer(examples[\"review\"], max_length=128, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batch_size=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3c5a8-2c95-49ed-b695-c886022442fc",
   "metadata": {},
   "source": [
    "其實他一樣是使用pytorch Dataset，只是又被Transformers多包一層，其中train是預設值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b9eb7bc-6f8f-4865-b9c2-5a71762063bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 64\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "trainset, validset = tokenized_datasets[\"train\"], tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, \n",
    "                         collate_fn=DataCollatorWithPadding(tokenizer, \n",
    "                                            padding=True, # Pad to the longest sequence in the batch\n",
    "                                            return_tensors='pt'))\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False, \n",
    "                         collate_fn=DataCollatorWithPadding(tokenizer, padding=True, return_tensors='pt'))\n",
    "next_input = next(iter(validloader))\n",
    "print(f\"batch size {len(next(iter(next_input.values())))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d082e2-2060-440c-883c-aac5dd7c815d",
   "metadata": {},
   "source": [
    "## 手動訓練 (參照/01-Getting Started/05-datasets/)\n",
    "\n",
    "`model.train()`   \n",
    "`model.eval()`   \n",
    " 開啟/關閉 Batch Normalization 和 Dropout\n",
    "\n",
    " \n",
    "`torch.inference_mode():`    \n",
    "https://stackoverflow.com/questions/55627780/evaluating-pytorch-models-with-torch-no-grad-vs-model-eval    \n",
    "不追蹤梯度，減少資源使用    \n",
    "相當於    \n",
    "```python\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "\n",
    "`optimizer.zero_grad():`  \n",
    "為什麼要手動歸 0，可以累積幾輪才一次update  \n",
    "https://meetonfriday.com/posts/18392404/  \n",
    "\n",
    "\n",
    "評估函數請參照：  \n",
    "/01-Getting Started/06-evaluate/evaluate.ipynb  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c978fe1b-c343-41ed-b064-4399e8bb1052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 0, loss: 0.6433025002479553\n",
      "ep: 0, global_step: 100, loss: 0.2641475796699524\n",
      "ep: 0, global_step: 200, loss: 0.3857491910457611\n",
      "ep: 0, {'accuracy': 0.87001287001287, 'f1': 0.9130060292850991}|0.8700128793716431\n",
      "ep: 1, global_step: 300, loss: 0.19777831435203552\n",
      "ep: 1, global_step: 400, loss: 0.17106226086616516\n",
      "ep: 1, {'accuracy': 0.8944658944658944, 'f1': 0.9250457038391224}|0.8944659233093262\n",
      "ep: 2, global_step: 500, loss: 0.192722886800766\n",
      "ep: 2, global_step: 600, loss: 0.4233287572860718\n",
      "ep: 2, {'accuracy': 0.8931788931788932, 'f1': 0.924613987284287}|0.8931788802146912\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "# 如果是pipeline，就可以用device=0處理\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 手動算準確度\n",
    "def evaluate1():\n",
    "    # https://stackoverflow.com/questions/55627780/evaluating-pytorch-models-with-torch-no-grad-vs-model-eval\n",
    "    model.eval() # 關閉 Batch Normalization 和 Dropout\n",
    "    acc_num = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in validloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "            acc_num += (pred.long() == batch[\"labels\"].long()).float().sum()\n",
    "    return acc_num / len(validset)\n",
    "\n",
    "# 用現成lib算準確度\n",
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\"])\n",
    "def evaluate2():\n",
    "    model.eval() # 關閉 Batch Normalization 和 Dropout\n",
    "    with torch.inference_mode():\n",
    "        for batch in validloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            # SequenceClassifierOutput(loss=tensor(0.1632, device='cuda:0'), \n",
    "            #                            logits=tensor([[-0.6985,  1.4187],[xxx,xxx],....]]),\n",
    "            #                            device='cuda:0'), hidden_states=None, attentions=None\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "            clf_metrics.add_batch(predictions=pred.long(), references=batch[\"labels\"].long())\n",
    "    return clf_metrics.compute()\n",
    "    \n",
    "def train(epoch=3, log_step=100):\n",
    "    global_step = 0\n",
    "    for ep in range(epoch):\n",
    "        # model.train()會啟用 batch normalization 和 dropout, 如果模型中有BN层（Batch Normalization）和 Dropout.\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "             # 這邊是把training data放入gpu，所以不要跟前面的model.cuda搞混\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad() # 每輪將optimizer梯度歸0，除非你要累積多次\n",
    "            output = model(**batch) \n",
    "            # SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2049, -0.3266, .... -0.3472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            if global_step % log_step == 0:\n",
    "                print(f\"ep: {ep}, global_step: {global_step}, loss: {output.loss.item()}\")\n",
    "            global_step += 1\n",
    "        clf = evaluate2()\n",
    "        myacc = evaluate1()\n",
    "        clf.update()\n",
    "        print(f\"ep: {ep}, {clf}|{myacc}\")\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a21abf75-41df-418e-8800-cbf6101d32a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '好评！', 'score': 0.9802994728088379}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model.config.id2label = {0: \"差评！\", 1: \"好评！\"}\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe(\"店員很漂亮\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a94036-0818-44b5-8305-c5e3a2012ab0",
   "metadata": {},
   "source": [
    "## 自動訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b4ae6-cd5c-40ff-b76e-ab351dd259d0",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05bdab36-3e5e-4cf5-9773-2d1bc017abb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a051c1a1ad8499d8a01eac5d16612cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3ebc74824448e3b11b1132ab6f829b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/777 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"./01-Getting Started/05-datasets/ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
    "datasets = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "def process_function(examples):\n",
    "    # 一樣不需要padding跟return_tensors=\"pt\" (都給DataCollatorWithPadding處理)\n",
    "    tokenized_examples = tokenizer(examples[\"review\"], max_length=128, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befaf97-9c75-4b87-af75-be91c9e88d6c",
   "metadata": {},
   "source": [
    "### 2. Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8869b7b-b816-4396-8c84-7f4e886d55a9",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8afaaf62-61b1-4a36-97b3-c40f26929ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"hfl/rbt3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-2): 3 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\")\n",
    "print(model.config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93a588-f148-49c7-a982-16f627006c4a",
   "metadata": {},
   "source": [
    "#### Evaluater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1377c4fc-8bc2-4c9f-af8f-c5373136678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def eval_metric(eval_predict): # Trainer\n",
    "    predictions, labels = eval_predict\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels) # {\"accuracy\": 0.99},\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels) # {\"f1\": 0.95},\n",
    "    acc.update(f1) # {\"accuracy\": 0.99, \"f1\": 0.95}\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565898f-ab7b-4b8e-9109-8bb00593e35e",
   "metadata": {},
   "source": [
    "#### TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4b732f0c-381b-4217-817f-6c0e8d0aeca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=2,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=epoch,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=True,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./checkpoints/runs/Aug30_09-26-19_u22-dk-gp,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=10,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=f1,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./checkpoints,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=128,\n",
       "per_device_train_batch_size=64,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./checkpoints,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=3,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "train_args = TrainingArguments(output_dir=\"./checkpoints\",      # 输出文件夹\n",
    "                               per_device_train_batch_size=64,  # 训练时的batch_size\n",
    "                               per_device_eval_batch_size=128,  # 验证时的batch_size\n",
    "                               logging_steps=10,                # log 打印的频率\n",
    "                               evaluation_strategy=\"epoch\",     # 评估策略(多久評估一次)\n",
    "                               save_strategy=\"epoch\",           # 保存策略(多久保存一次)\n",
    "                               save_total_limit=3,              # 最大保存数\n",
    "                               learning_rate=2e-5,              # 学习率\n",
    "                               weight_decay=0.01,               # weight_decay\n",
    "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               load_best_model_at_end=True)     # 训练完成后加载最优模型\n",
    "train_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cd0de-133b-412d-a24b-e9ee423cbee8",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3a2b5274-b741-4bf2-9e35-b0461d5e9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "trainer = Trainer(model=model, \n",
    "                  args=train_args, \n",
    "                  train_dataset=tokenized_datasets[\"train\"], # 取代Dataloader\n",
    "                  eval_dataset=tokenized_datasets[\"test\"],  # 取代Dataloader\n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "                  compute_metrics=eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "06a8a3d8-b511-4c69-96c6-3400c0c73ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for transformers >= 4.42.4\n",
    "# ValueError: You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.weight` which is not allowed. \n",
    "# It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors,\n",
    "# and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving.\n",
    "\n",
    "# for name, param in model.bert.named_parameters():\n",
    "#     # 修正 contiguous 的 error\n",
    "#     if not param.is_contiguous():\n",
    "#         param.data = param.data.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4fc9df34-a932-401b-966b-fcd9fd21850a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [165/165 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.352700</td>\n",
       "      <td>0.293507</td>\n",
       "      <td>0.873874</td>\n",
       "      <td>0.907547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.260700</td>\n",
       "      <td>0.256122</td>\n",
       "      <td>0.888031</td>\n",
       "      <td>0.919220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.254943</td>\n",
       "      <td>0.886744</td>\n",
       "      <td>0.917603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/smith/miniconda3/envs/peft/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=165, training_loss=0.3320430827863289, metrics={'train_runtime': 66.3084, 'train_samples_per_second': 316.159, 'train_steps_per_second': 2.488, 'total_flos': 351909933963264.0, 'train_loss': 0.3320430827863289, 'epoch': 3.0})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9c2a0-671d-4003-abee-a9759961a047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ea4b5-5044-4c13-b4d9-32eb4819dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER (named entry recognization)\n",
    "BERT + dropout + linear (token num -> label)\n",
    "分類字節\n",
    "\n",
    "MRC (machine reading comprehension)\n",
    "1. QA (給予文章跟Q，輸出A)\n",
    "2. 答案輸出可能是填空、選擇、節錄、生成\n",
    "訓練資料收集： context + question + answer{answer_start在原文哪裡, text摘錄}\n",
    "如何輸入：`[CLS]Question[SEP]Context[SEP]`\n",
    "\n",
    "BERT + linear (hiddensize-> labels(=2, start&end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d49235-9b6e-4274-8761-7ee5a8c3c759",
   "metadata": {},
   "source": [
    "## 原文QA 訓練:    \n",
    "\n",
    "1. 將 question跟context整合成一個pair (BertTokenizerFast(text, text_pair))    \n",
    "     註：BertTokenizerFast相較一般版，可以提供`offsets_mapping`\n",
    "3. 找到answer位於該pair的何處，算出index加入example (即'start_positions', 'end_positions')     \n",
    "4. 使用QAModel訓練    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e7885f5-e5a1-472c-a58f-9e831e7c9c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='hfl/chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = load_dataset(\"cmrc2018\")\n",
    "sample_dataset = datasets[\"train\"].select(range(10))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "165181d0-d097-4e8d-9682-7c35279813f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BertTokenizerFast text, text_pair.\n",
    "tokenized_examples = tokenizer(text=sample_dataset[\"question\"],\n",
    "                               text_pair=sample_dataset[\"context\"],\n",
    "                                return_offsets_mapping=True, # <-是否生成offset_mapping (下面會說明)\n",
    "                               max_length=384, truncation=\"only_second\", # first是question \n",
    "                               padding=\"max_length\")\n",
    "tokenized_examples.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5a340-4595-428d-a8f5-0c524ddae664",
   "metadata": {},
   "source": [
    "這邊會把Q跟Context合併在一起:\n",
    "[CLS] 范 廷 颂 ... 任 为 主 教 的 ？ [SEP] 范 廷 颂 枢 机 ....同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "80c3b89d-6037-43d1-b94f-6e3c33126c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 [SEP]\n",
      "=== token_type_ids ====\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "=== offset_mapping ====\n",
      " [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 105), (105, 106), (106, 107), (107, 108), (108, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 116), (116, 117), (117, 118), (118, 119), (119, 120), (120, 121), (121, 122), (122, 123), (123, 124), (124, 125), (125, 126), (126, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 133), (133, 134), (134, 135), (135, 136), (136, 137), (137, 138), (138, 139), (139, 140), (140, 141), (141, 142), (142, 143), (143, 144), (144, 145), (145, 146), (146, 147), (147, 148), (148, 149), (149, 150), (150, 151), (151, 152), (152, 153), (153, 154), (154, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 163), (163, 164), (164, 165), (165, 166), (166, 167), (167, 168), (168, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 175), (175, 176), (176, 177), (177, 178), (178, 179), (179, 180), (180, 181), (181, 182), (182, 186), (186, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 192), (192, 193), (193, 194), (194, 195), (195, 196), (196, 197), (197, 198), (198, 199), (199, 200), (200, 201), (201, 202), (202, 203), (203, 204), (204, 205), (205, 206), (206, 207), (207, 208), (208, 209), (209, 210), (210, 211), (211, 212), (212, 213), (213, 214), (214, 215), (215, 216), (216, 217), (217, 218), (218, 222), (222, 223), (223, 224), (224, 225), (225, 226), (226, 227), (227, 228), (228, 229), (229, 230), (230, 231), (231, 232), (232, 233), (233, 234), (234, 235), (235, 236), (236, 237), (237, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 243), (243, 244), (244, 245), (245, 246), (246, 247), (247, 248), (248, 249), (249, 250), (250, 251), (251, 252), (252, 253), (253, 257), (257, 258), (258, 259), (259, 260), (260, 261), (261, 262), (262, 263), (263, 264), (264, 265), (265, 266), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 276), (276, 277), (277, 278), (278, 279), (279, 280), (280, 281), (281, 282), (282, 283), (283, 284), (284, 285), (285, 286), (286, 287), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (292, 293), (293, 294), (294, 295), (295, 296), (296, 297), (297, 298), (298, 299), (299, 300), (300, 301), (301, 302), (302, 303), (303, 304), (304, 305), (305, 306), (306, 307), (307, 308), (308, 309), (309, 310), (310, 311), (311, 312), (312, 313), (313, 314), (314, 315), (315, 316), (316, 317), (317, 318), (318, 319), (319, 320), (320, 321), (321, 325), (325, 326), (326, 327), (327, 328), (328, 329), (329, 330), (330, 331), (331, 332), (332, 333), (333, 334), (334, 335), (335, 336), (336, 337), (337, 338), (338, 339), (339, 340), (340, 341), (341, 342), (342, 343), (343, 344), (344, 345), (345, 346), (346, 347), (347, 348), (348, 349), (349, 350), (350, 351), (351, 352), (352, 353), (353, 354), (354, 355), (355, 356), (356, 360), (360, 361), (361, 362), (362, 363), (363, 364), (364, 365), (365, 366), (366, 367), (367, 368), (368, 369), (369, 370), (370, 371), (371, 372), (372, 373), (373, 374), (374, 375), (375, 376), (376, 377), (377, 378), (378, 379), (379, 380), (380, 381), (381, 382), (382, 383), (383, 384), (384, 385), (385, 386), (386, 387), (387, 388), (388, 390), (390, 391), (391, 392), (392, 393), (393, 394), (394, 395), (395, 396), (396, 397), (397, 398), (398, 399), (399, 400), (400, 401), (0, 0)] 384\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_examples[\"input_ids\"][0]))\n",
    "\n",
    "# 該token屬於question(type 0) or content(type 1)\n",
    "print(\"=== token_type_ids ====\\n\", tokenized_examples[\"token_type_ids\"][0])\n",
    "\n",
    "# offset_mapping => 每個token自己內部的index跟offset都是從(0,0)開始\n",
    "print(\"=== offset_mapping ====\\n\", tokenized_examples[\"offset_mapping\"][0], len(tokenized_examples[\"offset_mapping\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4f1c2-cc3b-4c12-8994-0aeb101b073a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
